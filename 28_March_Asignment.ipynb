{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9ec56c3",
   "metadata": {},
   "source": [
    "Q1->\n",
    "Ridge regression, also known as Tikhonov regularization, is a technique used in linear regression to mitigate the problem of multicollinearity and overfitting. It is an extension of ordinary least squares (OLS) regression.\n",
    "\n",
    "In ordinary least squares regression, the goal is to minimize the sum of squared residuals by estimating the coefficients that best fit the data. However, when the predictors in the model are highly correlated (multicollinearity), the OLS estimates can become unstable or highly sensitive to small changes in the data. This can lead to overfitting, where the model performs well on the training data but poorly on new, unseen data.\n",
    "\n",
    "Ridge regression addresses these issues by adding a penalty term to the OLS objective function. The penalty term, known as the L2 regularization term, is proportional to the squared magnitude of the coefficients. The objective of ridge regression is to find the coefficients that minimize the sum of squared residuals plus the regularization term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a1a43-aa02-4dd9-969b-d3c08612d8e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fe333c0-eaa0-4690-a03d-9028c1d64052",
   "metadata": {},
   "source": [
    "Q2->\n",
    "The assumptions of ridge regression are the same as that of linear regression: linearity, constant variance, and independence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbcaba2-4d53-4f41-b795-ce1fe6e41d82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8126b5cf-cfdc-4226-87a4-7e1b5a86204d",
   "metadata": {},
   "source": [
    "Q3->\n",
    "\n",
    "Selecting the value of the tuning parameter (lambda) in ridge regression requires finding the optimal balance between model complexity (flexibility) and the amount of shrinkage applied to the coefficients. Here are several common methods for choosing the value of lambda:\n",
    "\n",
    "Cross-Validation: One of the most widely used methods is to perform k-fold cross-validation on the training dataset. The process involves splitting the training data into k folds, fitting the ridge regression model on a subset of the folds, and evaluating its performance on the remaining fold. This process is repeated for different values of lambda, and the lambda value that yields the best average performance across all folds is selected.\n",
    "\n",
    "Grid Search: Another approach is to perform a grid search, where a predefined range of lambda values is specified. The ridge regression model is trained and evaluated for each value of lambda, and the lambda value that results in the best performance metric (e.g., mean squared error, R-squared) on a validation set or through cross-validation is chosen.\n",
    "\n",
    "RidgeCV: Some machine learning libraries provide built-in functions for automatic lambda selection. For example, scikit-learn's RidgeCV class performs cross-validation internally to determine the optimal lambda value. It searches for the lambda that minimizes the mean squared error or other specified metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008483f4-ae3b-4592-b412-63bae721f44a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00daa4fc-6800-4dc3-b18f-364d18bc3da1",
   "metadata": {},
   "source": [
    "Q4->\n",
    "Ridge regression can be used for feature selection, although it does not perform explicit variable selection by setting coefficients to exactly zero as in some other methods like LASSO regression. However, ridge regression can still be helpful in identifying and prioritizing important features by shrinking less relevant features towards zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10686c2-db09-4dfc-afdf-2b1f8dbbd93c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "912cb81b-1e8a-44b6-a48a-9d2bc1515661",
   "metadata": {},
   "source": [
    "Q5->\n",
    "\n",
    "When multicollinearity occurs, least squares estimates are unbiased, but their variances are large so they may be far from the true value. By adding a degree of bias to the regression estimates, ridge regression reduces the standard errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd4b2c0-f32a-4a2b-9c63-69c54bb77156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79c5d076-42ad-45ac-92b2-f071d3f4b8f2",
   "metadata": {},
   "source": [
    "Q6->\n",
    "\n",
    "Ridge regression can handle both categorical and continuous independent variables, but some preprocessing steps are necessary to appropriately incorporate categorical variables into the model.\n",
    "\n",
    "Categorical variables need to be transformed into numerical representations before they can be used in ridge regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b31f19-0732-44b6-8f31-7861338c4ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ee0302a-e1ab-4b56-8e7c-ab85d4eecf22",
   "metadata": {},
   "source": [
    "Q7->\n",
    "\n",
    "The ridge coefficients are a reduced factor of the simple linear regression coefficients and thus never attain zero values but very small values. The lasso coefficients become zero in a certain range and are reduced by a constant factor, which explains their low magnitude in comparison to the ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90efbf2a-4e29-485f-8956-28da3e116172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a77a4497-f5d9-4820-8626-630cabe931b5",
   "metadata": {},
   "source": [
    "Q8->\n",
    "\n",
    "Yes, Ridge Regression can be used for time-series data analysis, but it requires some modifications to account for the temporal nature of the data. Time-series data consists of observations collected over time, where the order and timing of the observations are crucial. Applying regular ridge regression directly to time-series data without considering the temporal dependencies can lead to incorrect results and violation of assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54992403-4651-4774-884d-def480b1e43e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
