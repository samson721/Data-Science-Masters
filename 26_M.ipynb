{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d689991",
   "metadata": {},
   "source": [
    "Q1->\n",
    "Simple linear regression involves modeling the relationship between a single independent variable (predictor) and a dependent variable. It assumes a linear relationship between the predictor and the target variable,  an example where we want to predict a student's exam score (Y) based on the number of hours they studied (X)\n",
    "\n",
    "Multiple linear regression involves modeling the relationship between a dependent variable and two or more independent variables. It assumes a linear relationship between the predictors and the target variable. \n",
    "Example of Multiple Linear Regression:\n",
    "we want to predict a house's sale price (Y) based on its size (X1), the number of bedrooms (X2), and the age of the house (X3). We collect data on 100 houses, including their sizes, number of bedrooms, ages, and sale prices. We can use multiple linear regression to model the relationship between these predictors and the sale price.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de71bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35c22022",
   "metadata": {},
   "source": [
    "Q2->\n",
    "Linear regression relies on several assumptions to ensure the validity and reliability of the model. Here are the key assumptions of linear regression:\n",
    "\n",
    "Linearity: The relationship between the independent variables and the dependent variable should be linear. This means that the effect of each predictor on the target variable is constant, and the overall relationship is additive.\n",
    "\n",
    "Independence: The observations in the dataset should be independent of each other. In other words, there should be no correlation or dependence between the residuals (errors) of different observations.\n",
    "\n",
    "Homoscedasticity: The variance of the residuals should be constant across all levels of the predictors. This assumption implies that the spread of the residuals should be consistent, indicating a consistent level of uncertainty in the predictions.\n",
    "\n",
    "Normality: The residuals should follow a normal distribution. This assumption allows for valid statistical inference and hypothesis testing. The normality assumption also applies to the distribution of the target variable conditional on the predictors.\n",
    "\n",
    "No Multicollinearity: The predictors should not be highly correlated with each other. Multicollinearity can lead to difficulties in estimating the coefficients accurately and interpreting their individual effects.\n",
    "\n",
    "To check whether these assumptions hold in a given dataset, various diagnostic techniques can be employed:\n",
    "\n",
    "Residual Analysis: Examine the residuals (the differences between the observed and predicted values) to assess linearity, homoscedasticity, and normality. Plotting the residuals against the predicted values, the predictors, or other relevant variables can help identify potential violations of these assumptions.\n",
    "\n",
    "Normality Test: Conduct tests, such as the Shapiro-Wilk test or Kolmogorov-Smirnov test, to assess the normality of the residuals. Additionally, visual tools like a histogram or a Q-Q plot can provide a visual indication of the normality assumption.\n",
    "\n",
    "Collinearity Assessment: Calculate correlation coefficients or variance inflation factors (VIF) to identify any potential multicollinearity among the predictors. High correlations or VIF values above a certain threshold (e.g., 5) may indicate the presence of multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d62365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d53e37cc",
   "metadata": {},
   "source": [
    "Q3->\n",
    "The intercept (β0) represen ts the estimated value of the dependent variable when all independent variables are zero. It is the value of the dependent variable when the predictors have no effect. In other words, it represents the expected value of the dependent variable when all predictors are absent. \n",
    "\n",
    "The slope coefficients represent the change in the dependent variable associated with a one-unit change in the corresponding independent variable while holding other variables constant. They indicate the average increase or decrease in the dependent variable for each unit change in the predictor, assuming all other predictors remain constant.\n",
    "Interpretation example:\n",
    "Continuing with the electricity bill example, let's assume the linear regression model's coefficient for household size (number of occupants) is β1 = 50. This means that, on average, for every additional person in the household, the monthly electricity bill is expected to increase by 50rs, assuming other factors remain constant.\n",
    "\n",
    "So, if the initial model predicts a monthly electricity bill of 100rs for a single-person household, a two-person household would be expected to have a predicted bill of rs150 (rs100 + rs50).\n",
    "\n",
    "It's important to note that interpretations should be made within the context of the specific data and model. The interpretations of the slope and intercept coefficients are based on the assumptions and limitations of the linear regression model and the specific domain or problem being analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232fd1bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cba41d3",
   "metadata": {},
   "source": [
    "Q4->\n",
    "Gradient descent is an optimization algorithm commonly used in machine learning to minimize the loss or cost function of a model. The goal of gradient descent is to iteratively adjust the model's parameters (coefficients or weights) in the direction that reduces the cost function and leads to better model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191aa3d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77212d9a",
   "metadata": {},
   "source": [
    "Q5->\n",
    "\n",
    "Multiple linear regression is an extension of simple linear regression that allows for the modeling of the relationship between a dependent variable and two or more independent variables. It assumes a linear relationship between the independent variables and the dependent variable, but with multiple predictors involved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e09d61e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a79258b",
   "metadata": {},
   "source": [
    "Q6->\n",
    "Multicollinearity refers to a situation in multiple linear regression where two or more independent variables are highly correlated with each other. It can cause issues in the regression model by making it difficult to determine the individual effects of the correlated variables and affecting the stability and interpretability of the regression coefficients.\n",
    "\n",
    "Multicollinearity can manifest in two forms:\n",
    "\n",
    "Perfect Multicollinearity: In this case, one or more independent variables can be expressed as a perfect linear combination of other variables. For example, if you have two independent variables X1 and X2, where X2 = 2*X1, then perfect multicollinearity exists.\n",
    "\n",
    "High Multicollinearity: Here, there is a strong correlation between the independent variables, but not a perfect linear relationship. This means that the variables are highly interrelated but not redundant.\n",
    "\n",
    "Detecting Multicollinearity:\n",
    "There are several methods to detect multicollinearity:\n",
    "\n",
    "Correlation Matrix: Calculate the correlation coefficients between all pairs of independent variables. High correlations, typically above a certain threshold (e.g., 0.7 or 0.8), indicate potential multicollinearity.\n",
    "\n",
    "Variance Inflation Factor (VIF): Calculate the VIF for each independent variable. VIF quantifies the extent to which a variable can be explained by other variables in the model. VIF values above a threshold (e.g., 5 or 10) suggest multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b828e971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aaba9278",
   "metadata": {},
   "source": [
    "Q7->\n",
    "\n",
    "Polynomial regression is an extension of linear regression that allows for modeling nonlinear relationships between the independent variables (predictors) and the dependent variable. While linear regression assumes a linear relationship between the predictors and the dependent variable, polynomial regression accommodates curves and nonlinear patterns.\n",
    "\n",
    "The polynomial regression model can be represented as follows:\n",
    "\n",
    "Y = β0 + β1X + β2X^2 + β3X^3 + ... + βnX^n + ε\n",
    "\n",
    "The key difference between linear regression and polynomial regression is the inclusion of polynomial terms in the latter. While linear regression assumes a straight-line relationship between the predictors and the dependent variable, polynomial regression can capture more complex relationships, such as quadratic (U-shaped or inverted U-shaped), cubic, or higher-order curves.\n",
    "\n",
    "The choice of the degree of the polynomial (n) depends on the nature of the relationship between the predictors and the dependent variable. A higher degree polynomial can capture more intricate curves but may also introduce overfitting if the model becomes too complex for the available data. On the other hand, a lower degree polynomial may fail to capture important nonlinear patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca5ec0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3638e0e3",
   "metadata": {},
   "source": [
    "Q8->\n",
    "\n",
    "Advantages of Polynomial Regression over Linear Regression:\n",
    "\n",
    "Flexibility in Modeling Nonlinear Relationships: Polynomial regression can capture nonlinear patterns and relationships between the predictors and the dependent variable. It allows for more flexible modeling when the relationship between the variables is not strictly linear.\n",
    "\n",
    "Improved Fit to Data: By incorporating polynomial terms, polynomial regression can better fit the data points compared to linear regression. It can capture curvature and nonlinearity in the data, resulting in a potentially better fit and higher predictive accuracy.\n",
    "\n",
    "Disadvantages of Polynomial Regression compared to Linear Regression:\n",
    "\n",
    "Increased Complexity: As the degree of the polynomial increases, the model becomes more complex and may risk overfitting the data. Higher-order polynomials can introduce excessive complexity and noise, leading to a poor generalization to new data.\n",
    "\n",
    "Interpretability: Polynomial regression models can be more challenging to interpret compared to linear regression. The coefficients of polynomial terms represent the effect of the predictors on the dependent variable, but their interpretation becomes more complex as the degree of the polynomial increases.\n",
    "\n",
    "Extrapolation Challenge: Polynomial regression can be problematic when it comes to extrapolation, especially with higher-degree polynomials. The model may produce unreliable predictions outside the range of the observed data, leading to potential inaccuracies and invalid conclusions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9716c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
