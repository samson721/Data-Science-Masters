{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd314ae8",
   "metadata": {},
   "source": [
    "Q1 In linear regression analysis, the concept of R-squared (also known as the coefficient of determination) is used to evaluate the goodness-of-fit of the regression model to the observed data. R-squared measures the proportion of the variance in the dependent variable that can be explained by the independent variables included in the model.\n",
    "\n",
    "To calculate R-squared, you need to compare the total sum of squares (TSS) and the residual sum of squares (RSS). TSS represents the total variation in the dependent variable, while RSS represents the unexplained or residual variation. The formula for R-squared is as follows:\n",
    "\n",
    "R-squared = 1 - (RSS / TSS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb951d5",
   "metadata": {},
   "source": [
    "Q2->\n",
    "\n",
    "Adjusted R-squared is a modified version of the regular R-squared that takes into account the number of independent variables in the linear regression model. It adjusts the R-squared value to penalize the inclusion of unnecessary variables that may artificially inflate the regular R-squared.\n",
    "\n",
    "While the regular R-squared (R²) measures the proportion of the variance in the dependent variable that is explained by the independent variables, adjusted R-squared (R²_adj) considers the complexity of the model by adjusting for the number of predictors and the sample size. It provides a more conservative evaluation of the model's goodness-of-fit.\n",
    "\n",
    "The formula for adjusted R-squared is as follows:\n",
    "\n",
    "R²_adj = 1 - [(1 - R²) * (n - 1) / (n - p - 1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f0d3f0",
   "metadata": {},
   "source": [
    "Q3->\n",
    "\n",
    "adjusted R-squared is particularly useful when you want to compare regression models with different numbers of predictors, avoid overfitting, select the most relevant variables, and account for sample size considerations. It offers a more cautious and balanced evaluation of the model's goodness-of-fit and helps in making more informed decisions about model selection and interpretation.\n",
    "\n",
    "Also whwn the independent features are very lowly corelated to target or output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b416cb",
   "metadata": {},
   "source": [
    "Q4->\n",
    "\n",
    "Mean Squared Error (MSE):\n",
    "MSE calculates the mean of the squared differences between predicted and actual values. It gives an average measure of the prediction error, with larger errors being penalized more heavily due to the squaring operation. The formula for MSE is as follows:\n",
    "\n",
    "MSE = (1/n) * Σ(y - ŷ)²\n",
    "\n",
    "Mean Absolute Error (MAE):\n",
    "MAE calculates the mean of the absolute differences between predicted and actual values. It provides a measure of the average magnitude of the prediction error without considering the direction of the errors. The formula for MAE is as follows:\n",
    "\n",
    "MAE = (1/n) * Σ|y - ŷ|\n",
    "\n",
    "Root Mean Squared Error (RMSE):\n",
    "RMSE is a popular metric that calculates the square root of the mean of the squared differences between predicted and actual values. It provides a measure of the average magnitude of the prediction error. The formula for RMSE is as follows:\n",
    "\n",
    "RMSE = √(MSE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685e2f2c",
   "metadata": {},
   "source": [
    "Q5->\n",
    "\n",
    "MSE \n",
    "Advanatges: 1. diffrentaible 2.cit has 1 local and 1 global minima\n",
    "disadvantage: 1. Not Robust to outliers 2. Not in same unit\n",
    "\n",
    "    \n",
    "MAE\n",
    "Advantage: 1. in the same unit 2. Robust to outliers\n",
    "Distavantage 1. convergence takes time  2. optimization is complex\n",
    "\n",
    "RMSE\n",
    "Advantage: 1. in the same unit 2. Deffrentiable\n",
    "Disadvantage: 1. not robust to outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7774382c",
   "metadata": {},
   "source": [
    "Q6->\n",
    "\n",
    "Lasso regularization, also known as L1 regularization, is a technique used in regression analysis to introduce a penalty term based on the absolute values of the regression coefficients. It encourages sparsity by driving some of the coefficients to exactly zero, effectively performing feature selection.\n",
    "\n",
    "The key difference between Lasso regularization and Ridge regularization (L2 regularization) lies in the penalty term added to the regression objective function:\n",
    "\n",
    "Lasso Regularization:\n",
    "In Lasso regularization, the penalty term is the sum of the absolute values of the regression coefficients multiplied by a regularization parameter (λ). The objective function of Lasso regression is as follows:\n",
    "minimize: SSE + λ * Σ|β|\n",
    "    \n",
    "Ridge Regularization:\n",
    "In Ridge regularization, the penalty term is the sum of the squared values of the regression coefficients multiplied by a regularization parameter (λ). The objective function of Ridge regression is as follows:\n",
    "minimize: SSE + λ * Σ(β²)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa536715",
   "metadata": {},
   "source": [
    "Q7->\n",
    "\n",
    "Regularized linear models, such as Ridge regression and Lasso regression, help prevent overfitting in machine learning by introducing a penalty term that discourages overly complex or over-parameterized models. By controlling the size of the regression coefficients, regularization techniques provide a balance between model flexibility and simplicity, reducing the risk of overfitting. Here's an example to illustrate this concept:\n",
    "\n",
    "Let's consider a scenario where you have a dataset with 100 observations and 50 features (predictors). You want to build a linear regression model to predict a continuous target variable. However, you suspect that some of the predictors may be irrelevant or redundant, leading to potential overfitting if included in the model.\n",
    "\n",
    "Without regularization, you could fit a standard linear regression model that estimates the coefficients by minimizing the sum of squared errors (SSE) between the predicted and actual values. However, this approach may lead to overfitting when dealing with high-dimensional data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0607e1",
   "metadata": {},
   "source": [
    "Q8->\n",
    "While regularized linear models, such as Ridge regression and Lasso regression, offer several benefits, they also have limitations that may make them less suitable for certain regression analysis scenarios. Here are some limitations to consider:\n",
    "\n",
    "Interpretability: Regularized linear models may sacrifice interpretability, especially when using Lasso regression. Lasso tends to shrink some coefficients to exactly zero, effectively removing those predictors from the model. While this simplifies the model, it can make the interpretation more challenging, as some relevant predictors may be excluded.\n",
    "\n",
    "Sensitivity to Parameter Tuning: Regularized linear models require tuning the regularization parameter (λ) to control the amount of regularization applied. The choice of λ is crucial, and finding the optimal value often involves cross-validation or other techniques. If the tuning process is not done carefully, the performance of the model may be suboptimal, leading to underfitting or overfitting.\n",
    "\n",
    "Handling Multicollinearity: While Ridge regression helps mitigate multicollinearity by reducing the impact of correlated predictors, it does not eliminate it entirely. If multicollinearity is severe, Ridge regression may still struggle to provide accurate coefficient estimates. In such cases, other techniques specifically designed to handle multicollinearity, such as principal component regression or partial least squares regression, may be more appropriate.\n",
    "\n",
    "Nonlinear Relationships: Regularized linear models assume linear relationships between predictors and the target variable. If the underlying relationships are nonlinear, regularized linear models may not capture them effectively. In such cases, nonlinear regression models or more flexible machine learning algorithms like decision trees or neural networks may be better suited.\n",
    "\n",
    "Data Limitations: Regularized linear models require a sufficient amount of data to estimate the coefficients reliably. If the dataset is small, regularization may have a significant impact on the coefficient estimates, potentially resulting in less stable or less accurate models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f1ab23",
   "metadata": {},
   "source": [
    "Q9->\n",
    "\n",
    "Model B has a lower MAE (8) compared to Model A's RMSE of 10. Therefore, based on the given metrics, Model B is the better performer, as it has a smaller average absolute difference between predicted and actual values.\n",
    "\n",
    "However, it's important to consider the limitations of the chosen metric (MAE) when making the decision. Some limitations of MAE include:\n",
    "\n",
    "Ignoring the Direction of Errors: MAE treats over-prediction and under-prediction equally, without considering the direction of the errors. In some cases, the direction of the errors may be important, and RMSE can provide a more nuanced understanding by penalizing larger errors more heavily.\n",
    "\n",
    "Sensitivity to Outliers: MAE is less sensitive to outliers compared to RMSE, as it does not involve squaring the errors. If there are significant outliers with large errors in the dataset, RMSE might provide a more accurate representation of the overall performance.\n",
    "\n",
    "Different Magnitude Interpretation: The interpretation of MAE (mean absolute difference) and RMSE (root mean squared difference) is different. RMSE is on the same scale as the target variable, while MAE is an absolute difference and lacks the same contextual interpretation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ce9a00",
   "metadata": {},
   "source": [
    "Q10->\n",
    "Ridge Regularization (Model A, λ = 0.1):\n",
    "Ridge regularization is generally effective in reducing the impact of multicollinearity and stabilizing coefficient estimates.\n",
    "The value of λ = 0.1 suggests a moderate level of regularization, which strikes a balance between simplicity and prediction performance.\n",
    "Ridge regularization does not force coefficients to zero but shrinks them towards zero. This means that all predictors are included in the model, but their impact is reduced.\n",
    "Lasso Regularization (Model B, λ = 0.5):\n",
    "Lasso regularization has the advantage of performing feature selection by driving some coefficients exactly to zero.\n",
    "The value of λ = 0.5 suggests a relatively stronger level of regularization, potentially leading to more coefficients being set to zero.\n",
    "Lasso regularization is useful when there is a suspicion that some predictors are irrelevant or redundant. It creates sparse models, improving interpretability and reducing overfitting.\n",
    "In terms of model performance, the choice between Model A and Model B depends on the specific requirements and trade-offs of the analysis:\n",
    "\n",
    "If the goal is to reduce the impact of multicollinearity and stabilize the coefficients while including all predictors, Model A (Ridge regularization) may be more appropriate.\n",
    "If the goal is to perform feature selection and create a more interpretable and potentially simpler model by excluding irrelevant predictors, Model B (Lasso regularization) may be preferred.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3979cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
